{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Ingest and Clean: Core Sources\n",
        "\n",
        "This notebook pulls and lightly cleans initial datasets for a small Gulf Coast subset to enable fast iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw: /Users/liamguest/LProjects/AURA/AURA/data/raw\n",
            "Interim: /Users/liamguest/LProjects/AURA/AURA/data/interim\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "PROJECT_ROOT = Path('/Users/liamguest/LProjects/AURA/AURA')\n",
        "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
        "DATA_INTERIM = PROJECT_ROOT / 'data' / 'interim'\n",
        "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
        "DATA_INTERIM.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Raw:', DATA_RAW)\n",
        "print('Interim:', DATA_INTERIM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenFEMA Individual Assistance (IA) - Minimal Pull\n",
        "\n",
        "Filters: Texas and Louisiana, last ~15 years. Adjust as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'STATE_ABBR' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m start_year = dt.date.today().year - \u001b[32m15\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Build state list once to avoid nested f-string braces\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m states_str = \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m.join(STATE_ABBR)\n\u001b[32m      8\u001b[39m query = (\n\u001b[32m      9\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m$filter=state%20in%20(\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstates_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m)%20and%20incidentBeginDate%20ge%20\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_year\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-01-01\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m&$top=10000&$format=json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOPENFEMA_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mNameError\u001b[39m: name 'STATE_ABBR' is not defined"
          ]
        }
      ],
      "source": [
        "import datetime as dt\n",
        "\n",
        "OPENFEMA_URL = 'https://www.fema.gov/api/open/v2/IndividualAssistanceApplications'\n",
        "start_year = dt.date.today().year - 15\n",
        "\n",
        "# Build state list once to avoid nested f-string braces\n",
        "states_str = \"','\".join(STATE_ABBR)\n",
        "query = (\n",
        "    f\"$filter=state%20in%20('{states_str}')%20and%20incidentBeginDate%20ge%20'{start_year}-01-01'\"\n",
        "    \"&$top=10000&$format=json\"\n",
        ")\n",
        "url = f\"{OPENFEMA_URL}?{query}\"\n",
        "print(url)\n",
        "\n",
        "resp = requests.get(url, timeout=60)\n",
        "resp.raise_for_status()\n",
        "ia_json = resp.json()\n",
        "\n",
        "ia = pd.json_normalize(ia_json.get('IndividualAssistanceApplications', []))\n",
        "print(ia.shape)\n",
        "\n",
        "ia_out = DATA_RAW / 'openfema_ia_tx_la_ms_al_fl.json'\n",
        "ia.to_json(ia_out, orient='records', lines=False)\n",
        "print('Wrote', ia_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NOAA HURDAT2 - Best Track Data\n",
        "\n",
        "We will download the Atlantic basin text file and parse into a tabular format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HURDAT_URL = 'https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2023-070924.txt'\n",
        "hurdat_path = DATA_RAW / 'hurdat2_atlantic.txt'\n",
        "\n",
        "r = requests.get(HURDAT_URL, timeout=60)\n",
        "r.raise_for_status()\n",
        "hurdat_path.write_bytes(r.content)\n",
        "print('Wrote', hurdat_path)\n",
        "\n",
        "# Quick parse sketch: collect header and record lines\n",
        "lines = hurdat_path.read_text().splitlines()\n",
        "records = []\n",
        "current_storm = None\n",
        "for line in lines:\n",
        "    if line and line[0].isalpha():\n",
        "        # Header: e.g., AL011851, UNNAMED, 14\n",
        "        parts = [p.strip() for p in line.split(',')]\n",
        "        current_storm = {'id': parts[0], 'name': parts[1], 'n': int(parts[2])}\n",
        "    else:\n",
        "        # Data line\n",
        "        parts = [p.strip() for p in line.split(',')]\n",
        "        if len(parts) >= 8 and current_storm:\n",
        "            ymdh = parts[0]\n",
        "            rec = {\n",
        "                'storm_id': current_storm['id'],\n",
        "                'storm_name': current_storm['name'],\n",
        "                'date': ymdh[:8],\n",
        "                'time': ymdh[8:],\n",
        "                'record_id': parts[2],\n",
        "                'status': parts[3],\n",
        "                'lat': parts[4],\n",
        "                'lon': parts[5],\n",
        "                'max_wind_kt': parts[6],\n",
        "                'min_pres_mb': parts[7]\n",
        "            }\n",
        "            records.append(rec)\n",
        "\n",
        "hurdat_df = pd.DataFrame(records)\n",
        "hurdat_csv = DATA_INTERIM / 'hurdat2_atlantic_parsed.csv'\n",
        "hurdat_df.to_csv(hurdat_csv, index=False)\n",
        "print('Parsed records:', len(hurdat_df), '->', hurdat_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ACS Demographics (Census API) - Tract Level\n",
        "\n",
        "We will fetch a small set of variables for TX/LA tracts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CENSUS_BASE = 'https://api.census.gov/data/2022/acs/acs5'\n",
        "# Example variables: total population, median household income\n",
        "vars_ = ['NAME', 'B01003_001E', 'B19013_001E']\n",
        "get = ','.join(['GEO_ID'] + vars_)\n",
        "\n",
        "params = {\n",
        "    'get': get,\n",
        "    'for': 'tract:*',\n",
        "}\n",
        "\n",
        "acs_frames = []\n",
        "for state in STATE_FIPS:\n",
        "    p = params | {'in': f'state:{state}'}\n",
        "    if CENSUS_API_KEY:\n",
        "        p['key'] = CENSUS_API_KEY\n",
        "    r = requests.get(CENSUS_BASE, params=p, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    df = pd.DataFrame(r.json()[1:], columns=r.json()[0])\n",
        "    df['state_fips'] = state\n",
        "    acs_frames.append(df)\n",
        "\n",
        "acs = pd.concat(acs_frames, ignore_index=True)\n",
        "acs_out = DATA_INTERIM / 'acs_2022_tx_la_ms_al_fl.csv'\n",
        "acs.to_csv(acs_out, index=False)\n",
        "print('Wrote', acs_out, 'rows:', len(acs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration: five Gulf states and API key\n",
        "\n",
        "Defines state lists (TX, LA, MS, AL, FL) and reads your Census API key from the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "STATE_ABBR = ['TX', 'LA', 'MS', 'AL', 'FL']\n",
        "STATE_FIPS = ['48', '22', '28', '01', '12']\n",
        "CENSUS_API_KEY = os.getenv('CENSUS_API_KEY')\n",
        "print('CENSUS_API_KEY set:', bool(CENSUS_API_KEY))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CDC Social Vulnerability Index (SVI) - Placeholder\n",
        "\n",
        "Download the latest SVI (tract-level) for TX, LA, MS, AL, FL from CDC/ATSDR. Place CSVs under `data/raw/svi/` and run the next cell to combine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svi_dir = DATA_RAW / 'svi'\n",
        "svi_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Expect one or multiple CSVs dropped here; we will read all CSVs and concat\n",
        "svi_frames = []\n",
        "for p in sorted(svi_dir.glob('*.csv')):\n",
        "    try:\n",
        "        df = pd.read_csv(p, dtype=str)\n",
        "        df['source_file'] = p.name\n",
        "        svi_frames.append(df)\n",
        "    except Exception as e:\n",
        "        print('Failed to read', p, e)\n",
        "\n",
        "if svi_frames:\n",
        "    svi = pd.concat(svi_frames, ignore_index=True)\n",
        "    svi_out = DATA_INTERIM / 'svi_combined.csv'\n",
        "    svi.to_csv(svi_out, index=False)\n",
        "    print('Wrote', svi_out, 'rows:', len(svi))\n",
        "else:\n",
        "    print('No SVI CSVs found in', svi_dir)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aura",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

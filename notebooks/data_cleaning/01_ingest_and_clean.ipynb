{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Ingest and Clean: Core Sources\n",
        "\n",
        "This notebook pulls and lightly cleans initial datasets for a small Gulf Coast subset to enable fast iteration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw: /Users/liamguest/LProjects/AURA/AURA/data/raw\n",
            "Interim: /Users/liamguest/LProjects/AURA/AURA/data/interim\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "PROJECT_ROOT = Path('/Users/liamguest/LProjects/AURA/AURA')\n",
        "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
        "DATA_INTERIM = PROJECT_ROOT / 'data' / 'interim'\n",
        "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
        "DATA_INTERIM.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Raw:', DATA_RAW)\n",
        "print('Interim:', DATA_INTERIM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenFEMA Individual Assistance (IA) - Minimal Pull\n",
        "\n",
        "Filters: LargeDisastersDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FEMA rows: 25000 -> data/raw/fema_housing_subset.json\n",
            "Wrote clean subset: data/interim/fema_housing_subset_clean.csv rows: 25000\n"
          ]
        }
      ],
      "source": [
        "import time, requests, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "OPENFEMA_HOUSING_V1 = \"https://www.fema.gov/api/open/v1/IndividualAssistanceHousingRegistrantsLargeDisasters\"\n",
        "RAW = Path(\"data/raw\"); INT = Path(\"data/interim\"); RAW.mkdir(parents=True, exist_ok=True); INT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "STATE_ABBR = ['TX','LA','MS','AL','FL']\n",
        "states_str = \"','\".join(STATE_ABBR)\n",
        "\n",
        "# Smaller column set\n",
        "select_cols = \",\".join([\n",
        "  \"id\",\"disasterNumber\",\"damagedStateAbbreviation\",\"damagedZipCode\",\"ownRent\",\"residenceType\",\n",
        "  \"grossIncome\",\"specialNeeds\",\"tsaEligible\",\"repairAssistanceEligible\",\"replacementAssistanceEligible\",\n",
        "  \"personalPropertyEligible\",\"ppfvl\",\"censusBlockId\",\"censusYear\"\n",
        "])\n",
        "\n",
        "top = 5000           # reduce page size\n",
        "max_pages = 5        # limit pages for a fast first pass (â‰ˆ up to 25k rows)\n",
        "skip = 0\n",
        "frames = []\n",
        "\n",
        "for page in range(max_pages):\n",
        "    params = {\n",
        "        \"$filter\": f\"damagedStateAbbreviation in ('{states_str}')\",\n",
        "        \"$select\": select_cols,\n",
        "        \"$format\": \"json\",\n",
        "        \"$top\": top,\n",
        "        \"$skip\": skip\n",
        "    }\n",
        "    r = requests.get(OPENFEMA_HOUSING_V1, params=params, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    data = r.json().get(\"IndividualAssistanceHousingRegistrantsLargeDisasters\", [])\n",
        "    if not data:\n",
        "        break\n",
        "    frames.append(pd.DataFrame(data))\n",
        "    skip += top\n",
        "    time.sleep(0.2)\n",
        "\n",
        "fema = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
        "raw_out = RAW / \"fema_housing_subset.json\"\n",
        "fema.to_json(raw_out, orient=\"records\")\n",
        "print(\"FEMA rows:\", len(fema), \"->\", raw_out)\n",
        "\n",
        "def to_tract_geoid(val):\n",
        "    s = str(val) if pd.notna(val) else \"\"\n",
        "    return s[:11] if len(s) >= 11 else None\n",
        "\n",
        "if not fema.empty:\n",
        "    fema[\"tract_geoid\"] = fema[\"censusBlockId\"].map(to_tract_geoid)\n",
        "    clean_out = INT / \"fema_housing_subset_clean.csv\"\n",
        "    fema.to_csv(clean_out, index=False)\n",
        "    print(\"Wrote clean subset:\", clean_out, \"rows:\", len(fema))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25000, 16)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>disasterNumber</th>\n",
              "      <th>damagedStateAbbreviation</th>\n",
              "      <th>damagedZipCode</th>\n",
              "      <th>ownRent</th>\n",
              "      <th>residenceType</th>\n",
              "      <th>grossIncome</th>\n",
              "      <th>specialNeeds</th>\n",
              "      <th>tsaEligible</th>\n",
              "      <th>repairAssistanceEligible</th>\n",
              "      <th>replacementAssistanceEligible</th>\n",
              "      <th>personalPropertyEligible</th>\n",
              "      <th>ppfvl</th>\n",
              "      <th>censusBlockId</th>\n",
              "      <th>censusYear</th>\n",
              "      <th>tract_geoid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d214ed1e-951e-484b-b014-2a885a7ea234</td>\n",
              "      <td>4332</td>\n",
              "      <td>TX</td>\n",
              "      <td>77036</td>\n",
              "      <td>Renter</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1800.0</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.820143e+14</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>4.820143e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>e7b10c4f-adec-4a58-a4fc-85b5a20dfa4c</td>\n",
              "      <td>4337</td>\n",
              "      <td>FL</td>\n",
              "      <td>34238</td>\n",
              "      <td>Renter</td>\n",
              "      <td>Condo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.211500e+14</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>1.211500e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0832cc95-c445-4429-956c-e2d1864d37ac</td>\n",
              "      <td>4337</td>\n",
              "      <td>FL</td>\n",
              "      <td>34758</td>\n",
              "      <td>Renter</td>\n",
              "      <td>House/Duplex</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.209704e+14</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>1.209704e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ce6f1140-3777-41d8-8da0-f7f4891b2228</td>\n",
              "      <td>4559</td>\n",
              "      <td>LA</td>\n",
              "      <td>70663</td>\n",
              "      <td>Renter</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>55000.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.201900e+14</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>2.201900e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>f47b5c56-74b6-4572-844b-64aaacb85234</td>\n",
              "      <td>4332</td>\n",
              "      <td>TX</td>\n",
              "      <td>77088</td>\n",
              "      <td>Renter</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>23000.0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.820153e+14</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>4.820153e+10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     id  disasterNumber  \\\n",
              "0  d214ed1e-951e-484b-b014-2a885a7ea234            4332   \n",
              "1  e7b10c4f-adec-4a58-a4fc-85b5a20dfa4c            4337   \n",
              "2  0832cc95-c445-4429-956c-e2d1864d37ac            4337   \n",
              "3  ce6f1140-3777-41d8-8da0-f7f4891b2228            4559   \n",
              "4  f47b5c56-74b6-4572-844b-64aaacb85234            4332   \n",
              "\n",
              "  damagedStateAbbreviation  damagedZipCode ownRent residenceType  grossIncome  \\\n",
              "0                       TX           77036  Renter     Apartment       1800.0   \n",
              "1                       FL           34238  Renter         Condo          NaN   \n",
              "2                       FL           34758  Renter  House/Duplex      28000.0   \n",
              "3                       LA           70663  Renter     Apartment      55000.0   \n",
              "4                       TX           77088  Renter     Apartment      23000.0   \n",
              "\n",
              "   specialNeeds  tsaEligible  repairAssistanceEligible  \\\n",
              "0          True        False                     False   \n",
              "1         False        False                     False   \n",
              "2         False         True                     False   \n",
              "3         False        False                     False   \n",
              "4         False         True                     False   \n",
              "\n",
              "   replacementAssistanceEligible  personalPropertyEligible  ppfvl  \\\n",
              "0                          False                     False    NaN   \n",
              "1                          False                     False    NaN   \n",
              "2                          False                     False    0.0   \n",
              "3                          False                     False    NaN   \n",
              "4                          False                     False    0.0   \n",
              "\n",
              "   censusBlockId  censusYear   tract_geoid  \n",
              "0   4.820143e+14      2010.0  4.820143e+10  \n",
              "1   1.211500e+14      2010.0  1.211500e+10  \n",
              "2   1.209704e+14      2010.0  1.209704e+10  \n",
              "3   2.201900e+14      2020.0  2.201900e+10  \n",
              "4   4.820153e+14      2010.0  4.820153e+10  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"data/interim/fema_housing_subset_clean.csv\")\n",
        "print(df.shape)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NOAA HURDAT2 - Best Track Data\n",
        "\n",
        "We will download the Atlantic basin text file and parse into a tabular format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HURDAT_URL = 'https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2023-070924.txt'\n",
        "hurdat_path = DATA_RAW / 'hurdat2_atlantic.txt'\n",
        "\n",
        "r = requests.get(HURDAT_URL, timeout=60)\n",
        "r.raise_for_status()\n",
        "hurdat_path.write_bytes(r.content)\n",
        "print('Wrote', hurdat_path)\n",
        "\n",
        "# Quick parse sketch: collect header and record lines\n",
        "lines = hurdat_path.read_text().splitlines()\n",
        "records = []\n",
        "current_storm = None\n",
        "for line in lines:\n",
        "    if line and line[0].isalpha():\n",
        "        # Header: e.g., AL011851, UNNAMED, 14\n",
        "        parts = [p.strip() for p in line.split(',')]\n",
        "        current_storm = {'id': parts[0], 'name': parts[1], 'n': int(parts[2])}\n",
        "    else:\n",
        "        # Data line\n",
        "        parts = [p.strip() for p in line.split(',')]\n",
        "        if len(parts) >= 8 and current_storm:\n",
        "            ymdh = parts[0]\n",
        "            rec = {\n",
        "                'storm_id': current_storm['id'],\n",
        "                'storm_name': current_storm['name'],\n",
        "                'date': ymdh[:8],\n",
        "                'time': ymdh[8:],\n",
        "                'record_id': parts[2],\n",
        "                'status': parts[3],\n",
        "                'lat': parts[4],\n",
        "                'lon': parts[5],\n",
        "                'max_wind_kt': parts[6],\n",
        "                'min_pres_mb': parts[7]\n",
        "            }\n",
        "            records.append(rec)\n",
        "\n",
        "hurdat_df = pd.DataFrame(records)\n",
        "hurdat_csv = DATA_INTERIM / 'hurdat2_atlantic_parsed.csv'\n",
        "hurdat_df.to_csv(hurdat_csv, index=False)\n",
        "print('Parsed records:', len(hurdat_df), '->', hurdat_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ACS Demographics (Census API) - Tract Level\n",
        "\n",
        "We will fetch a small set of variables for TX/LA tracts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CENSUS_BASE = 'https://api.census.gov/data/2022/acs/acs5'\n",
        "# Example variables: total population, median household income\n",
        "vars_ = ['NAME', 'B01003_001E', 'B19013_001E']\n",
        "get = ','.join(['GEO_ID'] + vars_)\n",
        "\n",
        "params = {\n",
        "    'get': get,\n",
        "    'for': 'tract:*',\n",
        "}\n",
        "\n",
        "acs_frames = []\n",
        "for state in STATE_FIPS:\n",
        "    p = params | {'in': f'state:{state}'}\n",
        "    if CENSUS_API_KEY:\n",
        "        p['key'] = CENSUS_API_KEY\n",
        "    r = requests.get(CENSUS_BASE, params=p, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    df = pd.DataFrame(r.json()[1:], columns=r.json()[0])\n",
        "    df['state_fips'] = state\n",
        "    acs_frames.append(df)\n",
        "\n",
        "acs = pd.concat(acs_frames, ignore_index=True)\n",
        "acs_out = DATA_INTERIM / 'acs_2022_tx_la_ms_al_fl.csv'\n",
        "acs.to_csv(acs_out, index=False)\n",
        "print('Wrote', acs_out, 'rows:', len(acs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration: five Gulf states and API key\n",
        "\n",
        "Defines state lists (TX, LA, MS, AL, FL) and reads your Census API key from the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "STATE_ABBR = ['TX', 'LA', 'MS', 'AL', 'FL']\n",
        "STATE_FIPS = ['48', '22', '28', '01', '12']\n",
        "CENSUS_API_KEY = os.getenv('CENSUS_API_KEY')\n",
        "print('CENSUS_API_KEY set:', bool(CENSUS_API_KEY))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CDC Social Vulnerability Index (SVI) - Placeholder\n",
        "\n",
        "Download the latest SVI (tract-level) for TX, LA, MS, AL, FL from CDC/ATSDR. Place CSVs under `data/raw/svi/` and run the next cell to combine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svi_dir = DATA_RAW / 'svi'\n",
        "svi_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Expect one or multiple CSVs dropped here; we will read all CSVs and concat\n",
        "svi_frames = []\n",
        "for p in sorted(svi_dir.glob('*.csv')):\n",
        "    try:\n",
        "        df = pd.read_csv(p, dtype=str)\n",
        "        df['source_file'] = p.name\n",
        "        svi_frames.append(df)\n",
        "    except Exception as e:\n",
        "        print('Failed to read', p, e)\n",
        "\n",
        "if svi_frames:\n",
        "    svi = pd.concat(svi_frames, ignore_index=True)\n",
        "    svi_out = DATA_INTERIM / 'svi_combined.csv'\n",
        "    svi.to_csv(svi_out, index=False)\n",
        "    print('Wrote', svi_out, 'rows:', len(svi))\n",
        "else:\n",
        "    print('No SVI CSVs found in', svi_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Applications/anaconda3/envs/aura/bin/python 3.11.11\n",
            "Has CENSUS_API_KEY: False\n"
          ]
        }
      ],
      "source": [
        "import os, sys, platform\n",
        "print(sys.executable, platform.python_version())\n",
        "print('Has CENSUS_API_KEY:', bool(os.getenv('CENSUS_API_KEY')))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aura",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
